{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cntk \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cntk.learners import sgd , learning_rate_schedule , UnitType \n",
    "from cntk.logging import ProgressPrinter\n",
    "from cntk.layers import Dense , Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784)\n",
      "(55000, 10)\n",
      "(10000, 784)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print np.shape(mnist.train.images)\n",
    "print np.shape(mnist.train.labels)\n",
    "print np.shape(mnist.test.images)\n",
    "print np.shape(mnist.test.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_random_data(sample_size , feature_dim , num_classes):\n",
    "    y = np.random.randint( size=(sample_size , 1 )  , low=0 , high = num_classes )\n",
    "    x = (np.random.randn(sample_size , feature_dim) + 3 * (y+1))\n",
    "    x = x.astype(np.float32)\n",
    "    \n",
    "    class_ind = [y == class_number for class_number in range(num_classes)]\n",
    "    y= np.asarray(np.hstack(class_ind) , dtype=np.float32)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a utility function to compute the moving average sum.\n",
    "# A more efficient implementation is possible with np.cumsum() function\n",
    "def moving_average(a, w=5):\n",
    "    if len(a) < w:\n",
    "        return a[:]    # Need to send a copy of the array\n",
    "    return [val if idx < w else sum(a[(idx-w):idx])/w for idx, val in enumerate(a)]\n",
    "\n",
    "\n",
    "# Defines a utility that prints the training progress\n",
    "def print_training_progress(trainer, mb, frequency, verbose=1):\n",
    "    training_loss = \"NA\"\n",
    "    eval_error = \"NA\"\n",
    "\n",
    "    if mb%frequency == 0:\n",
    "        training_loss = trainer.previous_minibatch_loss_average\n",
    "        eval_error = trainer.previous_minibatch_evaluation_average\n",
    "        if verbose:\n",
    "            print (\"Minibatch: {0}, Loss: {1:.4f}, Error: {2:.2f}%\".format(mb, training_loss, eval_error*100))\n",
    "\n",
    "    return mb, training_loss, eval_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.13840562 -0.39486715 -1.82903444]\n",
      " [-0.23992308 -0.34654054  1.1703136 ]\n",
      " [ 1.56885547  0.32146374 -0.53372104]]\n",
      "[[7]\n",
      " [7]\n",
      " [1]]\n",
      "[[ 22.74185795  24.08037805  23.62570173]\n",
      " [ 22.82202231  22.59268653  24.01240461]\n",
      " [  5.0079721    6.0367091    5.2138694 ]]\n"
     ]
    }
   ],
   "source": [
    "sample_size=3\n",
    "num_classes=10\n",
    "feature_dim=3\n",
    "y = np.random.randint( size=(sample_size , 1 )  , low=0 , high = num_classes )\n",
    "x = (np.random.randn(sample_size , feature_dim) + 3 * (y+1))\n",
    "print np.random.randn(sample_size , feature_dim)\n",
    "print y \n",
    "print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 10)\n",
      "(60, 784)\n"
     ]
    }
   ],
   "source": [
    "featrues=mnist.train.next_batch(60)\n",
    "print np.shape(featrues[1])\n",
    "print np.shape(featrues[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input('Input3', [#], [784])\n",
      "Input('Input4', [#], [10])\n",
      "Dense(x: Sequence[tensor]) -> Sequence[tensor]\n",
      "Dense(x: Sequence[tensor]) -> Sequence[tensor]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/cntk/core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input4\") expects \"<type 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Composite(keep: Sequence[tensor]) -> Sequence[tensor]\n",
      " average      since    average      since      examples\n",
      "    loss       last     metric       last              \n",
      " ------------------------------------------------------\n",
      "Learning rate per minibatch: 0.125\n",
      "     2.75       2.75      0.933      0.933            60\n",
      "Minibatch: 0, Loss: 2.7460, Error: 93.33%\n",
      "train features : (60, 784)\n",
      "labels : (60, 10)\n",
      "sample_count 60\n",
      "aggregate_loss 164.760940552\n",
      "     4.17       4.88      0.911        0.9           180\n",
      "     5.37       6.28      0.905        0.9           420\n",
      "     5.48       5.56       0.89      0.877           900\n",
      "     4.67       3.92      0.836      0.785          1860\n",
      "     3.67       2.69      0.757       0.68          3780\n",
      "      2.5       1.35      0.577      0.401          7620\n",
      "     1.62      0.754      0.403      0.229         15300\n",
      "Minibatch: 500, Loss: 0.3216, Error: 10.00%\n",
      "     1.06      0.497      0.274      0.146         30660\n",
      "Minibatch: 1000, Loss: 0.3402, Error: 11.67%\n",
      "    0.734      0.409      0.198      0.122         61380\n",
      "Minibatch: 1500, Loss: 0.3253, Error: 10.00%\n",
      "Minibatch: 2000, Loss: 0.2812, Error: 10.00%\n",
      "    0.539      0.344      0.149      0.101        122820\n",
      "Minibatch: 2500, Loss: 0.3567, Error: 13.33%\n",
      "Minibatch: 3000, Loss: 0.3890, Error: 8.33%\n",
      "Minibatch: 3500, Loss: 0.2208, Error: 6.67%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/cntk/logging/progress_print.pyc\u001b[0m in \u001b[0;36mon_training_update_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m___write_progress_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregate_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregate_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mon_training_update_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0;31m# Override for ProgressWriter.on_training_update_end.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m___generate_progress_heartbeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    },
    {
     "ename": "RuntimeError",
     "evalue": "SWIG director method error.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0b4c12788d51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m98052\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mffnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-0b4c12788d51>\u001b[0m in \u001b[0;36mffnet\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mtrain_images\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mfeatures\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtrain_images\u001b[0m  \u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0msample_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprevious_minibatch_sample_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0maggregate_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprevious_minibatch_loss_average\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msample_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/cntk/train/trainer.pyc\u001b[0m in \u001b[0;36mtrain_minibatch\u001b[0;34m(self, arguments, outputs, device)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 updated = super(Trainer, self).train_minibatch(arguments,\n\u001b[0;32m--> 173\u001b[0;31m                     device)\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/cntk/cntk_py.pyc\u001b[0m in \u001b[0;36mtrain_minibatch\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   2805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2806\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2807\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_cntk_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer_train_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2809\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: SWIG director method error."
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "\"\"\"\n",
    "fully connected classifier network model function \n",
    "ce = cross_entropy which defined out model's loss function \n",
    "pe = classification error \n",
    "\"\"\"\n",
    "\n",
    "def ffnet():\n",
    "    inputs = 784\n",
    "    outputs = 10  \n",
    "    layers =2 \n",
    "    hidden_dimension=1024\n",
    "    features = cntk.input_variable((inputs) , np.float32)\n",
    "    print features\n",
    "    label = cntk.input_variable((outputs) , np.float32)\n",
    "    print label\n",
    "    \n",
    "    #[Dense(hidden_dimension ,activation=cntk.sigmoid)\n",
    "    #Composite(Dense): Placeholder('x', [???], [???]) -> Output('Block16_Output_0', [???], [???])\n",
    "    #Dense(outputs)\n",
    "    \n",
    "    print Dense(hidden_dimension ,activation=cntk.sigmoid)\n",
    "    print Dense(outputs)\n",
    "    my_model = Sequential(layers=[Dense(hidden_dimension ,activation=cntk.sigmoid),Dense(outputs)])\n",
    "    print my_model \n",
    "    z = my_model(features)\n",
    "    ce = cntk.cross_entropy_with_softmax(z , label)\n",
    "    pe = cntk.classification_error(z, label)    \n",
    "    \n",
    "    lr_per_minibatch = learning_rate_schedule(0.125 , UnitType.minibatch)\n",
    "\n",
    "    progress_printer = ProgressPrinter(0)\n",
    "    trainer = cntk.Trainer(model=z , criterion=(ce , pe) , \n",
    "                           parameter_learners=[sgd(z.parameters ,lr=lr_per_minibatch)] ,\n",
    "                       progress_writers=[progress_printer])\n",
    "    # z \n",
    "\n",
    "    minibatch_size=60\n",
    "    num_minibatches_to_train=300000\n",
    "    aggregate_loss = 0.0\n",
    "    s_time=time.time()\n",
    "    \n",
    "    for i in range(num_minibatches_to_train):\n",
    "\n",
    "        #if you want to use 'generate_random_data' for data , uncomment below line\n",
    "        #train_features , labels = generate_random_data(minibatch_size , inputs , outputs)\n",
    "\n",
    "        train_images , train_labels=mnist.train.next_batch(60)\n",
    "        trainer.train_minibatch({features : train_images  , label : train_labels })\n",
    "        sample_count = trainer.previous_minibatch_sample_count\n",
    "        aggregate_loss += trainer.previous_minibatch_loss_average * sample_count\n",
    "        print_training_progress(trainer, i, 500, verbose=1)   \n",
    "        if i ==0:\n",
    "            print 'train features :', np.shape(train_images)\n",
    "            print 'labels :' , np.shape(train_labels)\n",
    "            print 'sample_count',sample_count\n",
    "            print 'aggregate_loss',aggregate_loss\n",
    "        \n",
    "    print 'time : {}'.format(time.time()-s_time)\n",
    "    last_avg_error = aggregate_loss / trainer.total_number_of_samples_seen\n",
    "    #test_features , test_labels  = generate_random_data(minibatch_size , inputs , outputs)\n",
    "    test_features=mnist.test.images\n",
    "    test_labels = mnist.test.labels\n",
    "    avg_error = trainer.test_minibatch({features : test_features , label : test_labels })\n",
    "    print(' error rate on an unseen minibatch : {}'.format(avg_error))\n",
    "    return last_avg_error , avg_error\n",
    "    \n",
    "\n",
    "np.random.seed(98052)\n",
    "ffnet()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}